{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Project 3: Collaboration and Competition\n",
    "This file reports the method adopted to train two agents for solving the task of playing game of tennis in a Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- state space: 24 (Continuous)\n",
    "- action space: 2 (Continuous)\n",
    "- number of agents: 2\n",
    "- reward structure: \n",
    "    - Agent hits the ball over the net --> +0.1. \n",
    "    - Agent lets a ball hit the ground or hits the ball out of bounds --> -0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "- The inputs are given to an agent which follows a policy for executing the task. \n",
    "- As the state space has real numbers, implementing a Q table is not possible. Hence, neural networks are used for function approximation.\n",
    "- Also, as the action space is continuous, methods such as DQN cannot be directly implemented.\n",
    "- Moreover, here we have two agents learning the same task. To leverage the learning of both the agents performing same task, the Multi-Agent Deep Deterministic Policy Gradient (DDPG) algorithm (i.e. an Actor-Critic method) was used. \n",
    "    - Each agent in this method has its own DDPG model. \n",
    "        - The actor for each DDPG agent (i.e. policy) gets its own state and outputs actions.\n",
    "        - To faciliate learning, the Critic of the each agent not only gets its own state but also states of other agents. \n",
    "    - Also, the agents share the same memory (i.e. replay buffer) from which samples are drawn for training. \n",
    "- The structure of the Actor for each agent is as follows:\n",
    "    - F1 = ReLU (input_state (states = 24) x 256 neurons)\n",
    "    - F2 = ReLU (F1 x 256 neurons)\n",
    "    - F3 = ReLU (F2 x output_state (actions = 2))\n",
    "- The structure of the Critic for each agent is as follows:\n",
    "    - F1 = ReLU (input_state (states = 24) x 256 neurons)\n",
    "    - F2 = ReLU (F1 x 256 neurons)\n",
    "    - F3 = ReLU (F2 x 1)\n",
    "- Batch Normalization was added after the first layer (F1) for the Actor as well as Critic. It helps the networks to train faster. However, a second Batch Normalization after F2 did not help and was excluded from the final model.\n",
    "- For each agent, two NNs for actor and critic of same architecture are used: local network (θ_local) and target network (θ_target).\n",
    "- The target network is soft updated using the local network θ_target = τ*θ_local + (1 - τ)*θ_target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- BUFFER_SIZE = 1e5      # replay buffer size\n",
    "- BATCH_SIZE = 128       # minibatch size\n",
    "- GAMMA = 0.99           # discount factor\n",
    "- TAU = 1e-3             # for soft update of target parameters\n",
    "- LR_ACTOR = 1e-4        # Actor Learning Rate\n",
    "- LR_CRITIC = 1e-3       # Critic Learning Rate\n",
    "- maximum number of timesteps per episode = 1000\n",
    "- WEIGHT_DECAY = 0       # L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards plot\n",
    "A plot of the average rewards received is seen below:\n",
    "![alt text](images/reward_plot.png \"ABC\")\n",
    "It can be seen that the agent receives higher rewards as the experience i.e. number of episodes increases. \n",
    "There are episodes with scores as high as 2.5 but there are episodes with 0 scores as well.\n",
    "\n",
    "Number of episodes needed to solve the environment = 1408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future ideas for improving agents performance\n",
    "- Use a different Neural Network architecture for actor and critic\n",
    "- Implement prioritized experience replay for better sampling of episodes. Optimizing the sampling could improve the results as the positive rewards here are sparse. Also, implementing methods such as Hindsight Experience Replay for sparse rewards might help train the agent faster.\n",
    "- Another obvious method would be Proximal Policy Optimization (PPO) which might yield better results as this not very high-dimensional state space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2_drlnd",
   "language": "python",
   "name": "p2_drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
