{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Project 3: Collaboration and Competition\n",
    "This file reports the method adopted to train two agents for solving the task of playing game of tennis in a Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- state space: 24 (Continuous)\n",
    "- action space: 2 (Continuous)\n",
    "- number of agents: 2\n",
    "- reward structure: \n",
    "    - Agent hits the ball over the net --> +0.1. \n",
    "    - Agent lets a ball hit the ground or hits the ball out of bounds --> -0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "- The inputs are given to an agent which follows a policy for executing the task. \n",
    "- As the state space has real numbers, implementing a Q table is not possible. Hence, neural networks are used for function approximation.\n",
    "- Also, as the action space is continuous, methods such as DQN cannot be directly implemented.\n",
    "- Moreover, here we have two agents learning the same task. To leverage the learning both the agents, the Multi-Agent Deep Deterministic Policy Gradient (DDPG) algorithm which is an Actor-Critic method was used. \n",
    "    - The agents share the same memory (i.e. replay buffer) for training. \n",
    "    - The actor for each agent i.e. policy takes the state and outputs actions.\n",
    "    - The Critic on the other hand evaluates the expected values from a state,action pair i.e. Q value estimator.\n",
    "- The structure of the Actor for each agent is as follows:\n",
    "    - F1 = ReLU (input_state (states = 24) x 256 neurons)\n",
    "    - F2 = ReLU (F1 x 256 neurons)\n",
    "    - F3 = ReLU (F2 x output_state (actions = 2))\n",
    "- The structure of the Critic for each agent is as follows:\n",
    "    - F1 = ReLU (input_state (states = 24) x 256 neurons)\n",
    "    - F2 = ReLU (F1 x 256 neurons)\n",
    "    - F3 = ReLU (F2 x 1)\n",
    "- Two NNs for actor and critic of same architecture are used: local network (θ_local) and target network (θ_target).\n",
    "- The target network is soft updated using the local network θ_target = τ*θ_local + (1 - τ)*θ_target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- BUFFER_SIZE = 1e5      # replay buffer size\n",
    "- BATCH_SIZE = 128       # minibatch size\n",
    "- GAMMA = 0.99           # discount factor\n",
    "- TAU = 1e-3             # for soft update of target parameters\n",
    "- LR_ACTOR = 1e-4        # Actor Learning Rate\n",
    "- LR_CRITIC = 1e-3       # Critic Learning Rate\n",
    "- maximum number of timesteps per episode = 1000\n",
    "- WEIGHT_DECAY = 0       # L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards plot\n",
    "A plot of the average rewards received is seen below:\n",
    "![alt text](images/reward_plot.png \"ABC\")\n",
    "It can be seen that the agent receives higher rewards as the experience i.e. number of episodes increases. \n",
    "\n",
    "Number of episodes needed to solve the environment = 1408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future ideas for improving agents performance\n",
    "- Use a different Neural Network architecture for actor and critic\n",
    "- Implement with other methods such as A3C, PPO, D4PG for faster and improved agent performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2_drlnd",
   "language": "python",
   "name": "p2_drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
